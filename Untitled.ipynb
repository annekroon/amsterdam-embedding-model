{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fname_vermeer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"fname_vermeer.pkl\"\n",
    "s.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type \"<class 'int'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fe30aee8bb8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdict_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectorizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6690\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6691\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6692\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6694\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    287\u001b[0m                        \u001b[0;34m' only pd.Series, pd.DataFrame, and pd.Panel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                        ' (deprecated) objs are valid'.format(type(obj)))\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type \"<class 'int'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "dataset = 'burscher'\n",
    "import json\n",
    "with open(\"../tmpanne/AEM_output/baseline_true_predicted_dataset_{}_embed_size_large.json\".format(dataset)) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for k, v in data.items():\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(v)):\n",
    "        dict_new = v[str(i)]\n",
    "        df = df.append(dict_new, ignore_index=True)\n",
    "\n",
    "res = df.set_index(['classifier', 'model', 'vectorizer'])['actual'].apply(pd.Series).stack()\n",
    "res = res.reset_index()\n",
    "res.rename(columns={0: 'actual'}, inplace=True)\n",
    "res.drop(columns=['level_3'],inplace=True)\n",
    "\n",
    "res2 = df.set_index(['classifier', 'model', 'vectorizer'])['predicted'].apply(pd.Series).stack()\n",
    "res2 = res2.reset_index()\n",
    "res2.rename(columns={0: 'predicted'}, inplace = True)\n",
    "res2.drop(columns=['level_3'],inplace=True)\n",
    "\n",
    "df = pd.merge(res, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "dataset = 'burscher'\n",
    "\n",
    "dataset='vermeer'\n",
    "\n",
    "import json\n",
    "with open(\"../tmpanne/AEM_output/baseline_true_predicted_dataset_{}_embed_size_large.json\".format(dataset)) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for k, v in data.items():\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(v)):\n",
    "        dict_new = v[str(i)]\n",
    "        df = df.append(dict_new, ignore_index=True)\n",
    "\n",
    "res = df.set_index(['classifier', 'model', 'vectorizer'])['actual'].apply(pd.Series).stack()\n",
    "res = res.reset_index()\n",
    "res.rename(columns={0: 'actual'}, inplace=True)\n",
    "res.drop(columns=['level_3'],inplace=True)\n",
    "\n",
    "res2 = df.set_index(['classifier', 'model', 'vectorizer'])['predicted'].apply(pd.Series).stack()\n",
    "res2 = res2.reset_index()\n",
    "res2.rename(columns={0: 'predicted'}, inplace = True)\n",
    "res2.drop(columns=['level_3'],inplace=True)\n",
    "\n",
    "df = pd.merge(res, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ET\n",
       "1         ET\n",
       "2         ET\n",
       "3         ET\n",
       "4         ET\n",
       "5         ET\n",
       "6         ET\n",
       "7         ET\n",
       "8         ET\n",
       "9         ET\n",
       "10        ET\n",
       "11        ET\n",
       "12        ET\n",
       "13        ET\n",
       "14        ET\n",
       "15        ET\n",
       "16        ET\n",
       "17        ET\n",
       "18        ET\n",
       "19        ET\n",
       "20        ET\n",
       "21        ET\n",
       "22        ET\n",
       "23        ET\n",
       "24        ET\n",
       "25        ET\n",
       "26        ET\n",
       "27        ET\n",
       "28        ET\n",
       "29        ET\n",
       "          ..\n",
       "974378    ET\n",
       "974379    ET\n",
       "974380    ET\n",
       "974381    ET\n",
       "974382    ET\n",
       "974383    ET\n",
       "974384    ET\n",
       "974385    ET\n",
       "974386    ET\n",
       "974387    ET\n",
       "974388    ET\n",
       "974389    ET\n",
       "974390    ET\n",
       "974391    ET\n",
       "974392    ET\n",
       "974393    ET\n",
       "974394    ET\n",
       "974395    ET\n",
       "974396    ET\n",
       "974397    ET\n",
       "974398    ET\n",
       "974399    ET\n",
       "974400    ET\n",
       "974401    ET\n",
       "974402    ET\n",
       "974403    ET\n",
       "974404    ET\n",
       "974405    ET\n",
       "974406    ET\n",
       "974407    ET\n",
       "Name: classifier, Length: 974408, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import embeddingvectorizer\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec.load('/home/anne/tmpanne/fullsample/w2v_model_nr_0_window_5_size_100_negsample_5')\n",
    "embedding_mdl = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "embedding_vect = embeddingvectorizer.EmbeddingCountVectorizer(embedding_mdl, 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_pickle(\"../tmpanne/AEM_data/dataset_vermeer.pkl\")\n",
    "#logging.info(\"... loading the data...\\n\\nthis is length of the dataframe: {}\".format(len(df)))\n",
    "test_size = 0.2\n",
    "data = df['text']\n",
    "labels = df['topic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "   # ('to_dense', DenseTransformer()),\n",
    "    ('clf', LogisticRegressionCV())\n",
    "])\n",
    "\n",
    "#clf = LogisticRegressionCV()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15956\r\n",
      "-rw-rw-r-- 1 anne anne    21046 Feb 20 10:38 baseline_classreport_dataset_burscher_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne     1716 Mar 25 20:17 baseline_classreport_dataset_vermeer_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne   800665 Feb 20 10:38 baseline_true_predicted_dataset_burscher_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne   122046 Mar 25 19:42 baseline_true_predicted_dataset_vermeer_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne    16416 Dec 10 17:53 classreport.json\r\n",
      "-rw-rw-r-- 1 anne anne   299514 Feb 20 06:32 embeddings_classreport_dataset_burscher_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne   299436 Feb  3 19:47 embeddings_classreport_dataset_burscher.pkl.json\r\n",
      "-rw-rw-r-- 1 anne anne    29796 Mar 25 18:30 embeddings_classreport_dataset_vermeer_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne 12179899 Feb 20 06:32 embeddings_true_predicted_dataset_burscher_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne  1990794 Mar 25 18:30 embeddings_true_predicted_dataset_vermeer_embed_size_large.json\r\n",
      "-rw-rw-r-- 1 anne anne    84116 Feb  4 11:58 _instrinic_evaluation_large.json\r\n",
      "-rw-rw-r-- 1 anne anne     2776 Feb  4 11:58 _instrinic_evaluation_large.pkl\r\n",
      "-rw-rw-r-- 1 anne anne   455323 Dec 10 17:53 SML_predicted_actual_text_cleaned.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../tmpanne/AEM_output/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../tmpanne/AEM_output/baseline_true_predicted_dataset_vermeer_embed_size_large.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../tmpanne/AEM_output/baseline_true_predicted_dataset_vermeer_embed_size_large.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>classifier</th>\n",
       "      <th>model</th>\n",
       "      <th>predicted</th>\n",
       "      <th>vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>baseline</td>\n",
       "      <td>[3, 2, 1, 3, 1, 3, 3, 3, 1, 3, 2, 1, 3, 3, 3, ...</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>baseline</td>\n",
       "      <td>[3, 2, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, ...</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              actual  classifier     model  \\\n",
       "0  [3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...  GaussianNB  baseline   \n",
       "1  [3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...  GaussianNB  baseline   \n",
       "\n",
       "                                           predicted vectorizer  \n",
       "0  [3, 2, 1, 3, 1, 3, 3, 3, 1, 3, 2, 1, 3, 3, 3, ...      Count  \n",
       "1  [3, 2, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, ...      Tfidf  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for k, v in data.items():\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(v)):\n",
    "        print(i)\n",
    "        dict_new = v[str(i)]\n",
    "        df = df.append(dict_new, ignore_index=True)\n",
    "        \n",
    "res = df.set_index(['classifier', 'model', 'vectorizer'])['actual'].apply(pd.Series).stack()\n",
    "res = res.reset_index()\n",
    "res.rename(columns={0: 'actual'}, inplace=True)\n",
    "res.drop(columns=['level_3'],inplace=True)\n",
    "\n",
    "res2 = df.set_index(['classifier', 'model', 'vectorizer'])['predicted'].apply(pd.Series).stack()\n",
    "res2 = res2.reset_index()\n",
    "res2.rename(columns={0: 'predicted'}, inplace = True)\n",
    "res2.drop(columns=['level_3'],inplace=True)\n",
    "\n",
    "df = pd.merge(res, res2)\n",
    "#res2.replace(columns={'0':})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>classifier</th>\n",
       "      <th>model</th>\n",
       "      <th>predicted</th>\n",
       "      <th>vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>baseline</td>\n",
       "      <td>[3, 2, 1, 3, 1, 3, 3, 3, 1, 3, 2, 1, 3, 3, 3, ...</td>\n",
       "      <td>Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>baseline</td>\n",
       "      <td>[3, 2, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, ...</td>\n",
       "      <td>Tfidf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              actual  classifier     model  \\\n",
       "0  [3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...  GaussianNB  baseline   \n",
       "1  [3, 2, 1, 1, 1, 3, 4, 3, 1, 3, 3, 4, 3, 3, 3, ...  GaussianNB  baseline   \n",
       "\n",
       "                                           predicted vectorizer  \n",
       "0  [3, 2, 1, 3, 1, 3, 3, 3, 1, 3, 2, 1, 3, 3, 3, ...      Count  \n",
       "1  [3, 2, 1, 1, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, ...      Tfidf  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1e8ca43b9e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "df = json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'model': 'baseline', 'predicted': ['3', '2', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'model': 'baseline', 'predicted': ['3', '2', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  {'model': 'baseline', 'predicted': ['3', '2', ...\n",
       "1  {'model': 'baseline', 'predicted': ['3', '2', ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)\n",
    "\n",
    "pd.read_json(\"../tmpanne/AEM_output/baseline_true_predicted_dataset_vermeer_embed_size_large.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'predicted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e47bed2e5704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Predicted label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;31m#   actual = data.actual.apply(pd.Series).merge(data, right_index = True, left_index = True) \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0;31m#   .drop([\"predicted\"], axis = 1).melt(id_vars = ['classifier'], value_name = \"Actual label\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'predicted'"
     ]
    }
   ],
   "source": [
    "predicted = data.predicted.apply(pd.Series).merge(data, right_index = True, left_index = True) \\\n",
    ".drop([\"predicted\"], axis = 1).melt(id_vars = ['classifier'], value_name = \"Predicted label\")\n",
    "\n",
    " #   actual = data.actual.apply(pd.Series).merge(data, right_index = True, left_index = True) \\\n",
    " #   .drop([\"predicted\"], axis = 1).melt(id_vars = ['classifier'], value_name = \"Actual label\")\n",
    "\n",
    " #   df = pd.merge(predicted, actual, how = 'inner', left_index = True, right_index = True)\n",
    "#    df['Classifier'] = df['classifier_x']\n",
    "#    df = df[df.variable_x != 'actual']\n",
    "#    df = df[['Predicted label', 'Actual label', 'Classifier', 'model', 'vectorizer']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'predicted': ['3', '2', '1', '1', '1', '3', '1', '3', '1', '3', '3', '1', '3', '3', '3', '4', '2', '2', '3', '3', '1', '1', '3', '3', '4', '3', '2', '4', '3', '3', '3', '3', '3', '2', '3', '3', '3', '4', '3', '3', '3', '1', '3', '1', '4', '3', '3', '3', '3', '4', '1', '3', '3', '2', '3', '3', '3', '3', '3', '3', '3', '1', '4', '1', '3', '2', '3', '3', '3', '3', '2', '1', '3', '3', '1', '3', '3', '1', '3', '1', '4', '2', '3', '3', '3', '2', '2', '3', '3', '2', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '3', '3', '3', '1', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '4', '3', '1', '3', '4', '2', '3', '3', '3', '3', '1', '3', '3', '1', '3', '3', '1', '4', '3', '3', '1', '1', '1', '1', '3', '1', '3', '3', '3', '4', '3', '3', '2', '3', '3', '2', '3', '2', '3', '3', '3', '3', '3', '1', '2', '3', '3', '4', '1', '1', '1', '2', '1', '3', '3', '4', '1', '4', '3', '1', '3', '2', '1', '3', '2', '3', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '2', '3', '3', '3', '4', '3', '3', '1', '3', '3', '3', '1', '1', '2', '3', '4', '1', '2', '3', '4', '3', '3', '3', '2', '4', '1', '3', '3', '4', '3', '3', '4', '3', '1', '4', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '2', '3', '2', '4', '1', '1', '3', '4', '2', '2', '3', '3', '3', '3', '4', '3', '3', '3', '3', '3', '4', '2', '1', '3', '3', '1', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '2', '2', '3', '3', '2', '3', '1', '2', '3', '4', '3', '1', '1', '1', '1', '3', '1', '1', '3', '2', '3', '1', '3', '2', '3', '3', '3', '3', '3', '2', '1', '1', '3', '3', '3', '1', '3', '3', '3', '3', '2', '3', '1', '3', '3', '3', '3', '1', '1', '3', '3', '1', '1', '3', '3', '1', '4', '3', '3', '1', '4', '3', '2', '4', '3', '4', '3', '1', '2', '3', '1', '3', '3', '1', '3', '1', '3', '3', '3', '3', '3', '4', '2', '3', '3', '1', '3', '3', '3', '2', '3', '3', '1', '2', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '3', '2', '2', '3', '3', '1', '3', '3', '2', '3', '4', '3', '3', '2', '3', '3', '4', '2', '3', '1', '3', '2', '3', '3', '1', '4', '2', '3', '2', '2', '3', '3', '3', '1', '2', '3', '3', '3', '3', '1', '3', '3', '3', '2', '2', '1', '3', '3', '1', '3', '3', '3', '2', '3', '1', '2', '3', '3', '3', '3', '3', '3', '3', '3', '2', '4', '3', '2', '1', '4', '3', '3', '3', '1', '3', '1', '3', '2', '3', '1', '3', '4', '1', '1', '2', '3', '3', '2', '1', '2', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '4', '3', '3', '1', '3', '1', '2', '2', '3', '3', '3', '3', '3', '1', '3', '3', '2', '1', '2', '3', '3', '2', '3', '3', '2', '3', '1', '3', '1', '3', '3', '3', '3', '1', '1', '1', '4', '3', '2', '3', '3', '1', '4', '3', '2', '1', '4', '1', '3', '3', '4', '1', '2', '3', '2', '3', '4', '3', '4', '3', '2', '1', '1', '3', '1', '3', '1', '2', '1', '1', '3', '1', '3', '3', '3', '3', '4', '3', '1', '3', '3', '3', '3', '3', '2', '2', '1', '1', '3', '3', '3', '4', '1', '1', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '1', '3', '2', '1', '1', '3', '3', '4', '3', '2', '2', '1', '2', '3', '3', '3', '1', '1', '3', '1', '3', '3', '3', '3', '3', '4', '3', '3', '3', '1', '3', '3', '4', '3', '3', '3', '3', '3', '3', '1', '2', '2', '2', '3', '1', '3', '3', '2', '1', '4', '3', '1', '1', '4', '2', '3', '3', '3', '2', '3', '3', '3', '1', '3', '3', '2', '3', '3', '1', '3', '3', '3', '1', '2', '3', '1', '1', '2', '3', '2', '3', '2', '3', '3', '3', '1', '1', '1', '1', '2', '3', '3', '3', '3', '3', '3', '3', '2', '3'], 'model': 'baseline', 'actual': ['3', '2', '1', '1', '1', '3', '4', '3', '1', '3', '3', '4', '3', '3', '3', '2', '2', '2', '4', '3', '3', '3', '3', '1', '4', '3', '4', '4', '3', '2', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '1', '3', '2', '4', '3', '3', '3', '3', '4', '4', '1', '3', '2', '3', '4', '1', '3', '3', '3', '3', '2', '4', '1', '3', '2', '3', '1', '3', '3', '3', '3', '3', '3', '1', '3', '3', '3', '4', '1', '4', '3', '3', '3', '3', '2', '2', '3', '3', '2', '2', '3', '2', '3', '3', '2', '3', '3', '3', '3', '3', '4', '3', '3', '4', '3', '3', '3', '3', '3', '2', '2', '3', '1', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '3', '3', '4', '3', '1', '3', '4', '2', '4', '3', '1', '3', '1', '3', '2', '1', '3', '3', '3', '2', '3', '3', '1', '1', '1', '2', '4', '3', '4', '1', '3', '4', '2', '3', '2', '3', '4', '1', '4', '1', '3', '3', '3', '3', '3', '3', '2', '3', '2', '4', '1', '1', '1', '2', '1', '3', '2', '4', '1', '2', '3', '1', '3', '2', '1', '3', '2', '3', '3', '3', '1', '3', '3', '3', '3', '3', '1', '3', '3', '3', '3', '3', '4', '3', '3', '1', '3', '3', '2', '2', '1', '3', '3', '4', '1', '3', '2', '4', '3', '1', '4', '2', '4', '1', '3', '3', '4', '4', '3', '4', '3', '1', '2', '3', '3', '1', '3', '3', '3', '3', '2', '3', '3', '3', '1', '3', '2', '3', '1', '4', '1', '1', '3', '4', '2', '2', '3', '3', '3', '3', '4', '4', '3', '3', '3', '3', '2', '2', '1', '3', '3', '1', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '2', '2', '1', '3', '2', '3', '1', '1', '3', '2', '3', '4', '1', '1', '4', '3', '1', '1', '4', '2', '2', '1', '2', '2', '3', '3', '3', '3', '3', '4', '2', '2', '2', '3', '3', '1', '2', '3', '3', '3', '2', '4', '1', '3', '3', '4', '3', '1', '1', '3', '3', '1', '3', '3', '4', '1', '4', '3', '4', '1', '3', '3', '2', '3', '3', '4', '3', '2', '1', '3', '1', '3', '3', '2', '3', '1', '3', '3', '3', '3', '3', '3', '2', '3', '2', '1', '3', '3', '3', '4', '3', '2', '1', '2', '3', '3', '3', '2', '3', '2', '3', '2', '3', '3', '3', '3', '1', '2', '3', '3', '4', '1', '3', '4', '3', '3', '2', '3', '3', '2', '1', '3', '4', '3', '3', '1', '1', '4', '3', '3', '1', '3', '2', '3', '3', '1', '3', '2', '3', '3', '2', '2', '3', '3', '3', '2', '3', '3', '3', '3', '2', '1', '3', '3', '4', '3', '1', '3', '2', '3', '1', '2', '3', '3', '3', '1', '3', '2', '3', '1', '3', '1', '3', '2', '4', '4', '1', '3', '3', '3', '2', '1', '3', '2', '3', '4', '3', '3', '1', '1', '2', '3', '3', '2', '1', '2', '3', '1', '3', '3', '3', '3', '2', '3', '3', '3', '4', '2', '3', '1', '3', '1', '3', '2', '3', '3', '3', '2', '3', '2', '3', '3', '3', '1', '2', '3', '3', '2', '3', '2', '2', '2', '3', '3', '1', '3', '3', '3', '3', '3', '1', '1', '4', '3', '3', '3', '3', '2', '3', '2', '3', '1', '4', '3', '3', '3', '4', '1', '4', '3', '2', '3', '4', '3', '3', '3', '3', '1', '3', '3', '1', '3', '1', '1', '1', '1', '3', '1', '3', '3', '3', '3', '4', '3', '4', '3', '3', '3', '3', '4', '2', '2', '1', '1', '3', '3', '3', '4', '1', '1', '4', '1', '3', '3', '3', '3', '3', '4', '3', '2', '2', '3', '1', '1', '1', '3', '4', '4', '3', '2', '4', '1', '2', '3', '3', '3', '1', '3', '2', '1', '3', '3', '2', '2', '3', '4', '3', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '2', '2', '3', '2', '2', '1', '3', '2', '1', '4', '3', '1', '1', '3', '2', '3', '2', '3', '2', '3', '2', '3', '1', '4', '3', '2', '3', '3', '2', '3', '2', '3', '2', '2', '3', '1', '1', '2', '3', '1', '3', '4', '3', '3', '3', '1', '2', '1', '1', '2', '3', '3', '3', '3', '3', '3', '3', '4', '3'], 'vectorizer': 'Tfidf', 'classifier': 'GaussianNB'}, '0': {'predicted': ['3', '2', '1', '3', '1', '3', '3', '3', '1', '3', '2', '1', '3', '3', '3', '4', '2', '2', '4', '3', '1', '1', '3', '3', '4', '3', '2', '4', '3', '4', '3', '3', '3', '2', '4', '4', '3', '4', '3', '3', '3', '1', '3', '1', '4', '2', '3', '2', '2', '4', '4', '3', '3', '2', '3', '3', '1', '3', '3', '3', '4', '3', '4', '1', '2', '2', '3', '3', '4', '3', '2', '1', '1', '3', '1', '3', '3', '1', '3', '1', '4', '2', '3', '3', '3', '2', '2', '3', '3', '2', '2', '3', '4', '3', '3', '1', '3', '3', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '3', '3', '2', '3', '3', '3', '3', '2', '1', '3', '3', '3', '4', '3', '3', '1', '4', '3', '3', '1', '3', '4', '2', '3', '3', '3', '2', '1', '3', '3', '1', '3', '3', '1', '4', '3', '4', '1', '1', '1', '1', '3', '1', '3', '3', '3', '4', '3', '3', '2', '3', '2', '4', '3', '1', '3', '3', '3', '3', '3', '3', '3', '2', '1', '3', '1', '1', '1', '2', '1', '3', '3', '4', '1', '4', '3', '1', '2', '2', '1', '4', '2', '3', '3', '3', '3', '4', '3', '3', '3', '3', '3', '3', '2', '3', '3', '2', '4', '3', '3', '1', '3', '3', '3', '1', '1', '2', '3', '4', '1', '2', '2', '4', '3', '3', '3', '2', '4', '1', '3', '2', '4', '3', '2', '4', '3', '1', '4', '3', '2', '1', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '2', '3', '1', '4', '1', '1', '3', '4', '4', '3', '3', '3', '3', '3', '4', '3', '3', '4', '3', '3', '4', '2', '1', '3', '3', '1', '3', '3', '3', '1', '3', '2', '4', '3', '3', '3', '3', '2', '2', '3', '3', '2', '1', '1', '2', '3', '4', '3', '4', '1', '1', '3', '3', '1', '1', '3', '2', '2', '1', '3', '2', '4', '3', '2', '3', '3', '2', '3', '1', '3', '3', '3', '1', '3', '3', '3', '3', '2', '3', '1', '3', '3', '3', '3', '3', '1', '3', '3', '1', '1', '3', '3', '1', '4', '3', '4', '1', '4', '3', '2', '3', '4', '4', '3', '1', '2', '3', '1', '3', '3', '1', '4', '1', '3', '3', '3', '1', '3', '3', '2', '3', '3', '1', '3', '3', '3', '2', '2', '4', '2', '2', '4', '1', '3', '4', '2', '2', '3', '3', '3', '1', '3', '3', '3', '2', '2', '3', '3', '1', '3', '3', '2', '1', '4', '2', '3', '2', '3', '3', '4', '2', '3', '1', '3', '3', '1', '3', '1', '4', '2', '3', '2', '3', '3', '3', '4', '1', '2', '3', '2', '4', '3', '2', '3', '3', '3', '3', '2', '1', '3', '3', '1', '3', '3', '3', '2', '3', '3', '4', '3', '4', '3', '3', '3', '3', '2', '3', '2', '3', '3', '2', '1', '4', '3', '3', '3', '1', '3', '1', '3', '2', '3', '1', '3', '4', '1', '1', '2', '3', '3', '2', '1', '3', '3', '1', '3', '1', '3', '3', '3', '3', '2', '3', '4', '2', '3', '2', '3', '1', '3', '2', '4', '3', '3', '2', '3', '1', '3', '3', '2', '1', '2', '3', '1', '2', '3', '4', '2', '3', '1', '3', '1', '3', '3', '3', '1', '3', '1', '1', '3', '3', '2', '3', '3', '1', '3', '3', '3', '1', '4', '3', '4', '3', '4', '1', '2', '3', '2', '2', '4', '3', '2', '1', '2', '1', '1', '3', '1', '3', '1', '1', '1', '3', '4', '1', '3', '3', '4', '3', '4', '3', '1', '3', '3', '2', '3', '2', '2', '1', '1', '1', '3', '3', '4', '4', '1', '3', '3', '1', '3', '3', '3', '3', '3', '2', '3', '4', '1', '3', '2', '1', '1', '3', '3', '4', '3', '2', '2', '1', '2', '2', '1', '3', '1', '1', '3', '1', '2', '2', '1', '3', '3', '4', '3', '3', '3', '1', '3', '3', '4', '2', '2', '3', '3', '2', '3', '1', '2', '2', '2', '3', '4', '1', '3', '3', '1', '4', '3', '1', '1', '4', '2', '3', '3', '2', '2', '3', '1', '3', '3', '3', '3', '2', '3', '3', '1', '3', '3', '3', '1', '4', '3', '1', '4', '2', '3', '2', '1', '2', '4', '3', '3', '1', '1', '1', '3', '2', '3', '3', '4', '3', '1', '3', '3', '2', '3'], 'model': 'baseline', 'actual': ['3', '2', '1', '1', '1', '3', '4', '3', '1', '3', '3', '4', '3', '3', '3', '2', '2', '2', '4', '3', '3', '3', '3', '1', '4', '3', '4', '4', '3', '2', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '1', '3', '2', '4', '3', '3', '3', '3', '4', '4', '1', '3', '2', '3', '4', '1', '3', '3', '3', '3', '2', '4', '1', '3', '2', '3', '1', '3', '3', '3', '3', '3', '3', '1', '3', '3', '3', '4', '1', '4', '3', '3', '3', '3', '2', '2', '3', '3', '2', '2', '3', '2', '3', '3', '2', '3', '3', '3', '3', '3', '4', '3', '3', '4', '3', '3', '3', '3', '3', '2', '2', '3', '1', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '3', '3', '4', '3', '1', '3', '4', '2', '4', '3', '1', '3', '1', '3', '2', '1', '3', '3', '3', '2', '3', '3', '1', '1', '1', '2', '4', '3', '4', '1', '3', '4', '2', '3', '2', '3', '4', '1', '4', '1', '3', '3', '3', '3', '3', '3', '2', '3', '2', '4', '1', '1', '1', '2', '1', '3', '2', '4', '1', '2', '3', '1', '3', '2', '1', '3', '2', '3', '3', '3', '1', '3', '3', '3', '3', '3', '1', '3', '3', '3', '3', '3', '4', '3', '3', '1', '3', '3', '2', '2', '1', '3', '3', '4', '1', '3', '2', '4', '3', '1', '4', '2', '4', '1', '3', '3', '4', '4', '3', '4', '3', '1', '2', '3', '3', '1', '3', '3', '3', '3', '2', '3', '3', '3', '1', '3', '2', '3', '1', '4', '1', '1', '3', '4', '2', '2', '3', '3', '3', '3', '4', '4', '3', '3', '3', '3', '2', '2', '1', '3', '3', '1', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '2', '2', '1', '3', '2', '3', '1', '1', '3', '2', '3', '4', '1', '1', '4', '3', '1', '1', '4', '2', '2', '1', '2', '2', '3', '3', '3', '3', '3', '4', '2', '2', '2', '3', '3', '1', '2', '3', '3', '3', '2', '4', '1', '3', '3', '4', '3', '1', '1', '3', '3', '1', '3', '3', '4', '1', '4', '3', '4', '1', '3', '3', '2', '3', '3', '4', '3', '2', '1', '3', '1', '3', '3', '2', '3', '1', '3', '3', '3', '3', '3', '3', '2', '3', '2', '1', '3', '3', '3', '4', '3', '2', '1', '2', '3', '3', '3', '2', '3', '2', '3', '2', '3', '3', '3', '3', '1', '2', '3', '3', '4', '1', '3', '4', '3', '3', '2', '3', '3', '2', '1', '3', '4', '3', '3', '1', '1', '4', '3', '3', '1', '3', '2', '3', '3', '1', '3', '2', '3', '3', '2', '2', '3', '3', '3', '2', '3', '3', '3', '3', '2', '1', '3', '3', '4', '3', '1', '3', '2', '3', '1', '2', '3', '3', '3', '1', '3', '2', '3', '1', '3', '1', '3', '2', '4', '4', '1', '3', '3', '3', '2', '1', '3', '2', '3', '4', '3', '3', '1', '1', '2', '3', '3', '2', '1', '2', '3', '1', '3', '3', '3', '3', '2', '3', '3', '3', '4', '2', '3', '1', '3', '1', '3', '2', '3', '3', '3', '2', '3', '2', '3', '3', '3', '1', '2', '3', '3', '2', '3', '2', '2', '2', '3', '3', '1', '3', '3', '3', '3', '3', '1', '1', '4', '3', '3', '3', '3', '2', '3', '2', '3', '1', '4', '3', '3', '3', '4', '1', '4', '3', '2', '3', '4', '3', '3', '3', '3', '1', '3', '3', '1', '3', '1', '1', '1', '1', '3', '1', '3', '3', '3', '3', '4', '3', '4', '3', '3', '3', '3', '4', '2', '2', '1', '1', '3', '3', '3', '4', '1', '1', '4', '1', '3', '3', '3', '3', '3', '4', '3', '2', '2', '3', '1', '1', '1', '3', '4', '4', '3', '2', '4', '1', '2', '3', '3', '3', '1', '3', '2', '1', '3', '3', '2', '2', '3', '4', '3', '3', '3', '1', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '2', '2', '3', '2', '2', '1', '3', '2', '1', '4', '3', '1', '1', '3', '2', '3', '2', '3', '2', '3', '2', '3', '1', '4', '3', '2', '3', '3', '2', '3', '2', '3', '2', '2', '3', '1', '1', '2', '3', '1', '3', '4', '3', '3', '3', '1', '2', '1', '1', '2', '3', '3', '3', '3', '3', '3', '3', '4', '3'], 'vectorizer': 'Count', 'classifier': 'GaussianNB'}}\n"
     ]
    }
   ],
   "source": [
    "for k,v in data.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual label</th>\n",
       "      <th>Predicted label</th>\n",
       "      <th>classifier</th>\n",
       "      <th>classifier_y</th>\n",
       "      <th>variable_y</th>\n",
       "      <th>variable_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10022</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>334</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21030 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual label Predicted label  classifier classifier_y variable_y  \\\n",
       "0                3               3  GaussianNB   GaussianNB          0   \n",
       "1                3               3  GaussianNB   GaussianNB          0   \n",
       "10               3               3  GaussianNB   GaussianNB          0   \n",
       "100              1               3  GaussianNB   GaussianNB          3   \n",
       "1000             3               4  GaussianNB   GaussianNB         33   \n",
       "10000            3               3  GaussianNB   GaussianNB        333   \n",
       "10001            3               3  GaussianNB   GaussianNB        333   \n",
       "10002            3               3  GaussianNB   GaussianNB        333   \n",
       "10003            3               3  GaussianNB   GaussianNB        333   \n",
       "10004            3               3  GaussianNB   GaussianNB        333   \n",
       "10005            3               3  GaussianNB   GaussianNB        333   \n",
       "10006            3               3  GaussianNB   GaussianNB        333   \n",
       "10007            3               3  GaussianNB   GaussianNB        333   \n",
       "10008            3               3  GaussianNB   GaussianNB        333   \n",
       "10009            3               3  GaussianNB   GaussianNB        333   \n",
       "1001             3               4  GaussianNB   GaussianNB         33   \n",
       "10010            3               3  GaussianNB   GaussianNB        333   \n",
       "10011            3               3  GaussianNB   GaussianNB        333   \n",
       "10012            3               3  GaussianNB   GaussianNB        333   \n",
       "10013            3               3  GaussianNB   GaussianNB        333   \n",
       "10014            3               3  GaussianNB   GaussianNB        333   \n",
       "10015            3               3  GaussianNB   GaussianNB        333   \n",
       "10016            3               3  GaussianNB   GaussianNB        333   \n",
       "10017            3               3  GaussianNB   GaussianNB        333   \n",
       "10018            3               3  GaussianNB   GaussianNB        333   \n",
       "10019            3               3  GaussianNB   GaussianNB        333   \n",
       "1002             3               4  GaussianNB   GaussianNB         33   \n",
       "10020            4               4  GaussianNB   GaussianNB        334   \n",
       "10021            4               4  GaussianNB   GaussianNB        334   \n",
       "10022            4               4  GaussianNB   GaussianNB        334   \n",
       "...            ...             ...         ...          ...        ...   \n",
       "9972             3               1  GaussianNB   GaussianNB        332   \n",
       "9973             3               3  GaussianNB   GaussianNB        332   \n",
       "9974             3               1  GaussianNB   GaussianNB        332   \n",
       "9975             3               3  GaussianNB   GaussianNB        332   \n",
       "9976             3               1  GaussianNB   GaussianNB        332   \n",
       "9977             3               3  GaussianNB   GaussianNB        332   \n",
       "9978             3               3  GaussianNB   GaussianNB        332   \n",
       "9979             3               3  GaussianNB   GaussianNB        332   \n",
       "998              3               4  GaussianNB   GaussianNB         33   \n",
       "9980             3               1  GaussianNB   GaussianNB        332   \n",
       "9981             3               3  GaussianNB   GaussianNB        332   \n",
       "9982             3               1  GaussianNB   GaussianNB        332   \n",
       "9983             3               3  GaussianNB   GaussianNB        332   \n",
       "9984             3               3  GaussianNB   GaussianNB        332   \n",
       "9985             3               3  GaussianNB   GaussianNB        332   \n",
       "9986             3               3  GaussianNB   GaussianNB        332   \n",
       "9987             3               3  GaussianNB   GaussianNB        332   \n",
       "9988             3               3  GaussianNB   GaussianNB        332   \n",
       "9989             3               3  GaussianNB   GaussianNB        332   \n",
       "999              3               4  GaussianNB   GaussianNB         33   \n",
       "9990             3               3  GaussianNB   GaussianNB        333   \n",
       "9991             3               3  GaussianNB   GaussianNB        333   \n",
       "9992             3               3  GaussianNB   GaussianNB        333   \n",
       "9993             3               3  GaussianNB   GaussianNB        333   \n",
       "9994             3               3  GaussianNB   GaussianNB        333   \n",
       "9995             3               3  GaussianNB   GaussianNB        333   \n",
       "9996             3               3  GaussianNB   GaussianNB        333   \n",
       "9997             3               3  GaussianNB   GaussianNB        333   \n",
       "9998             3               3  GaussianNB   GaussianNB        333   \n",
       "9999             3               3  GaussianNB   GaussianNB        333   \n",
       "\n",
       "      variable_y  \n",
       "0              0  \n",
       "1              0  \n",
       "10             0  \n",
       "100            3  \n",
       "1000          33  \n",
       "10000        333  \n",
       "10001        333  \n",
       "10002        333  \n",
       "10003        333  \n",
       "10004        333  \n",
       "10005        333  \n",
       "10006        333  \n",
       "10007        333  \n",
       "10008        333  \n",
       "10009        333  \n",
       "1001          33  \n",
       "10010        333  \n",
       "10011        333  \n",
       "10012        333  \n",
       "10013        333  \n",
       "10014        333  \n",
       "10015        333  \n",
       "10016        333  \n",
       "10017        333  \n",
       "10018        333  \n",
       "10019        333  \n",
       "1002          33  \n",
       "10020        334  \n",
       "10021        334  \n",
       "10022        334  \n",
       "...          ...  \n",
       "9972         332  \n",
       "9973         332  \n",
       "9974         332  \n",
       "9975         332  \n",
       "9976         332  \n",
       "9977         332  \n",
       "9978         332  \n",
       "9979         332  \n",
       "998           33  \n",
       "9980         332  \n",
       "9981         332  \n",
       "9982         332  \n",
       "9983         332  \n",
       "9984         332  \n",
       "9985         332  \n",
       "9986         332  \n",
       "9987         332  \n",
       "9988         332  \n",
       "9989         332  \n",
       "999           33  \n",
       "9990         333  \n",
       "9991         333  \n",
       "9992         333  \n",
       "9993         333  \n",
       "9994         333  \n",
       "9995         333  \n",
       "9996         333  \n",
       "9997         333  \n",
       "9998         333  \n",
       "9999         333  \n",
       "\n",
       "[21030 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={\"classifier_x\": \"classifier\", \"variable_x\": \"variable_y\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegressionCV(Cs=10, class_weight=None, cv=None,\n",
       "                                      dual=False, fit_intercept=True,\n",
       "                                      intercept_scaling=1.0, l1_ratios=None,\n",
       "                                      max_iter=100, multi_class='auto',\n",
       "                                      n_jobs=None, penalty='l2',\n",
       "                                      random_state=None, refit=True,\n",
       "                                      scoring=None, solver='lbfgs', tol=0.0001,\n",
       "                                      verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"GaussianNB\"]\n",
    "\n",
    "parameters = [ \n",
    "            { 'clf__var_smoothing' : [1e-8, 1e-7, 1e-6, 1e-5, 1e-4] }  ]\n",
    "\n",
    "classifiers = [ GaussianNB() ]\n",
    "\n",
    "class_report = []\n",
    "results = []\n",
    "        \n",
    "    for vec, n in zip([CountVectorizer(), TfidfVectorizer()], [\"Count\", \"Tfidf\"]):\n",
    "\n",
    "        print(\"loaded the vectorizer: {}\\n\\n\\{}\".format(n, vec)) \n",
    "\n",
    "        for name, classifier, params in zip(names, classifiers, parameters):\n",
    "            my_dict = {}\n",
    "\n",
    "            logging.info(\"Starting gridsearch CV..\")\n",
    "            logging.info(\"Classifier name: {}\\n classifier:{}\\n params{}\\n\".format(name, classifier, params)) \n",
    "\n",
    "            #clf_pipe = Pipeline([ ('vect', vec), ('clf', classifier), ])\n",
    "            clf = classifier\n",
    "            clf_pipe = make_pipeline(vec, FunctionTransformer(lambda x: x.todense(), accept_sparse=True), classifier)\n",
    "\n",
    "            gs_clf = GridSearchCV(clf_pipe, param_grid=params, cv=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15b762e20f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# Try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,  TfidfTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import logging\n",
    "import json\n",
    "from sklearn.svm import SVC\n",
    "import embeddingvectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import gensim\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#path_to_embeddings='/home/anne/tmpanne/AEM_small_sample/test'\n",
    "\n",
    "class classifier_analyzer():\n",
    "    \n",
    "    def __init__(self, path_to_data, path_to_embeddings, dataset):\n",
    "        self.nmodel = 0\n",
    "        df = pd.read_pickle(path_to_data + dataset)\n",
    "        logging.info(\"... loading the data...\\n\\nthis is length of the dataframe: {}\".format(len(df)))\n",
    "        self.test_size = 0.2\n",
    "        self.data = df['text']\n",
    "        self.labels = df['topic']\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data, self.labels, test_size=self.test_size, random_state=42)\n",
    "        self.basepath = path_to_embeddings\n",
    "        self.names = [\"BernoulliNB\", \"Passive Agressive\", \"SGDClassifier\" , \"SVM\", \"ET\"]\n",
    "        self.parameters = [ \n",
    "                    { 'clf__alpha': (1e-2, 1e-3, 1e-5) } ,\n",
    "                    \n",
    "                    {'clf__loss': ('hinge', 'squared_hinge'),\n",
    "                    'clf__C': (0.01, 0.5, 1.0)   ,\n",
    "                    'clf__fit_intercept': (True, False) ,\n",
    "                    'clf__max_iter': (5 ,10 ,15) } ,\n",
    "\n",
    "                    {'clf__max_iter': (20, 30) ,\n",
    "                    'clf__alpha': (1e-2, 1e-3, 1e-5),\n",
    "                    'clf__penalty': ('l2', 'elasticnet')} ,\n",
    "\n",
    "                    {'clf__C': [1, 10, 100, 1000],\n",
    "                    'clf__gamma': [0.001, 0.0001],\n",
    "                    'clf__kernel': ['rbf', 'linear']},\n",
    "\n",
    "\n",
    "                    { \"clf__max_features\": ['auto', 'sqrt', 'log2'] }\n",
    "\n",
    "                     ]\n",
    "        self.classifiers = [BernoulliNB(),\n",
    "                            PassiveAggressiveClassifier(), \n",
    "                            SGDClassifier(),\n",
    "                            SVC(),\n",
    "                            ExtraTreesClassifier() ]\n",
    "      \n",
    "                     \n",
    " \n",
    "      \n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        '''yields a dict with one item. key is the filename, value the gensim model'''\n",
    "\n",
    "        filenames = [e for e in os.listdir(self.basepath) if not e.startswith('.')]\n",
    "\n",
    "        for fname in filenames:\n",
    "            model = {}\n",
    "            path = os.path.join(self.basepath, fname)\n",
    "            logging.info(\"\\nLoading gensim model\")\n",
    "\n",
    "            if fname.startswith('w2v'):\n",
    "                mod = gensim.models.Word2Vec.load(path)\n",
    "            else:\n",
    "                mod = gensim.models.KeyedVectors.load_word2vec_format(path)\n",
    "\n",
    "            model['gensimmodel'] = dict(zip(mod.wv.index2word, mod.wv.vectors))\n",
    "            model['filename'] = fname\n",
    "            self.nmodel +=1\n",
    "            logging.info(\"loaded gensim model nr {}, named: {}\".format(self.nmodel, model['filename']))\n",
    "            yield model\n",
    "\n",
    "\n",
    "    def get_vectorizer(self, vectorizer, model):\n",
    "        logging.info(\"the vectorizer is: {}\".format(vectorizer))\n",
    "        \n",
    "        vec = {}   \n",
    "        vec['filename'] = vectorizer\n",
    "        if vectorizer == 'w2v_count':\n",
    "            s = embeddingvectorizer.EmbeddingCountVectorizer(model['gensimmodel'], 'mean')\n",
    "        elif vectorizer == 'w2v_tfidf':\n",
    "            s = embeddingvectorizer.EmbeddingTfidfVectorizer(model['gensimmodel'], 'mean')\n",
    "        vec['vectorizer'] = s\n",
    "    \n",
    "        yield vec\n",
    "\n",
    "\n",
    "    def gridsearch_with_classifiers(self):\n",
    "        class_report = []\n",
    "        results = []\n",
    "        \n",
    "        for model in self.get_w2v_model():\n",
    "            for v in [\"w2v_count\", \"w2v_tfidf\"]:\n",
    "                for vec in self.get_vectorizer(v, model):\n",
    "                    print(\"loaded the vectorizer: {}\".format(vec['filename'])) \n",
    "                    for name, classifier, params in zip(self.names, self.classifiers, self.parameters):\n",
    "                        my_dict = {}\n",
    "                        \n",
    "                        logging.info(\"Starting gridsearch CV..\")\n",
    "                        logging.info(\"Classifier name: {}\\n\\n\\n\\n\\nModel name:{}\\n\\n\\n\\n\\nVectorizer: {}\\n\\n\\n\\n\\nParameter settings: {}\\n\".format(name, model['filename'], vec['filename'], params)) \n",
    "                        \n",
    "                        clf_pipe = Pipeline([ ('vect', vec['vectorizer']), ('clf', classifier), ])\n",
    "\n",
    "                        gs_clf = GridSearchCV(clf_pipe, param_grid=params, cv=2)\n",
    "                        clf = gs_clf.fit(self.X_train, self.y_train)\n",
    "                        score = clf.score(self.X_test, self.y_test)\n",
    "\n",
    "                        logging.info(\"{} score: {}\".format(name, score))\n",
    "                        #logging.info(\"{} are the best estimators\".format(clf.best_estimator_))\n",
    "\n",
    "                        results_to_dict = classification_report((clf.best_estimator_.predict(self.X_test)), self.y_test, output_dict= True)\n",
    "\n",
    "                        results_to_dict['classifier'] = name\n",
    "                        results_to_dict['parameters'] = clf.best_params_\n",
    "                        results_to_dict['vectorizer'] = vec['filename']\n",
    "                        results_to_dict['model'] = model['filename']\n",
    "\n",
    "                        logging.info(\"Created dictionary with classification report: \\n\\n{}\".format(results_to_dict))\n",
    "                        class_report.append(results_to_dict)\n",
    "                        \n",
    "                        y_hats = clf.predict(self.X_test)\n",
    "                        results.append({\"predicted\": y_hats,\n",
    "                                        \"actual\" : self.y_test.values  ,\n",
    "                                        \"classifier\": name, \n",
    "                                        \"vectorizer\":vec['filename'], \n",
    "                                        \"model\": model['filename'] } )\n",
    "      \n",
    "        return class_report, results\n",
    "    \n",
    "    def gridsearch_with_classifiers_baseline(self):\n",
    "        class_report = []\n",
    "        results = []\n",
    "        \n",
    "        for vec, n in zip([CountVectorizer(), TfidfVectorizer()], [\"Count\", \"Tfidf\"]):\n",
    "            \n",
    "            print(\"loaded the vectorizer: {}\\n\\n\\{}\".format(n, vec)) \n",
    "            \n",
    "            for name, classifier, params in zip(self.names, self.classifiers, self.parameters):\n",
    "                my_dict = {}\n",
    "\n",
    "                logging.info(\"Starting gridsearch CV..\")\n",
    "                logging.info(\"Classifier name: {}\\n classifier:{}\\n params{}\\n\".format(name, classifier, params)) \n",
    "\n",
    "                clf_pipe = Pipeline([ ('vect', vec), ('clf', classifier), ])\n",
    "                #clf_pipe = make_pipeline(vec, FunctionTransformer(lambda x: x.todense(), accept_sparse=True), classifier)\n",
    "\n",
    "                gs_clf = GridSearchCV(clf_pipe, param_grid=params, cv=2)\n",
    "                clf = gs_clf.fit(self.X_train, self.y_train)\n",
    "                score = clf.score(self.X_test, self.y_test)\n",
    "\n",
    "                logging.info(\"{} score: {}\".format(name, score))\n",
    "                logging.info(\"{} are the best estimators\".format(clf.best_estimator_))\n",
    "\n",
    "                results_to_dict = classification_report((clf.best_estimator_.predict(self.X_test)), self.y_test, output_dict= True)\n",
    "\n",
    "                results_to_dict['classifier'] = name\n",
    "                results_to_dict['parameters'] = clf.best_params_\n",
    "                results_to_dict['vectorizer'] = n\n",
    "                results_to_dict['model'] = \"baseline\"\n",
    "                \n",
    "                logging.info(\"Created dictionary with classification report: \\n\\n{}\".format(results_to_dict))\n",
    "                class_report.append(results_to_dict)\n",
    "\n",
    "                y_hats = clf.predict(self.X_test)\n",
    "                \n",
    "                results.append({\"predicted\": y_hats,\n",
    "                                \"actual\" : self.y_test.values  ,\n",
    "                                \"classifier\": name ,\n",
    "                                \"vectorizer\": n , \n",
    "                                \"model\": \"baseline\" } )\n",
    "                \n",
    "        return class_report, results\n",
    "    \n",
    "\n",
    "def clean_df_true_pred(results):\n",
    "    return pd.DataFrame.from_dict(results)\n",
    "\n",
    "#\"w2v_count\", \"w2v_tfidf\", \"count\", \"tfidf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the vectorizer: Count\n",
      "\n",
      "\\CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded the vectorizer: Tfidf\n",
      "\n",
      "\\TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "a = classifier_analyzer(path_to_data='/home/anne/tmpanne/AEM_data/', path_to_embeddings='/home/anne/tmpanne/fullsample/', dataset='dataset_vermeer.pkl')\n",
    "class_report, results = a.gridsearch_with_classifiers_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'1': {'f1-score': 0.7339449541284403,\n",
       "   'precision': 0.6504065040650406,\n",
       "   'recall': 0.8421052631578947,\n",
       "   'support': 95},\n",
       "  '2': {'f1-score': 0.5964912280701755,\n",
       "   'precision': 0.5483870967741935,\n",
       "   'recall': 0.6538461538461539,\n",
       "   'support': 104},\n",
       "  '3': {'f1-score': 0.847001223990208,\n",
       "   'precision': 0.9226666666666666,\n",
       "   'recall': 0.7828054298642534,\n",
       "   'support': 442},\n",
       "  '4': {'f1-score': 0.5714285714285715,\n",
       "   'precision': 0.5,\n",
       "   'recall': 0.6666666666666666,\n",
       "   'support': 57},\n",
       "  'accuracy': 0.7621776504297995,\n",
       "  'classifier': 'BernoulliNB',\n",
       "  'macro avg': {'f1-score': 0.687216494404349,\n",
       "   'precision': 0.6553650668764752,\n",
       "   'recall': 0.7363558783837421,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__alpha': 1e-05},\n",
       "  'vectorizer': 'Count',\n",
       "  'weighted avg': {'f1-score': 0.7717848537773647,\n",
       "   'precision': 0.7953288576179968,\n",
       "   'recall': 0.7621776504297995,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.832618025751073,\n",
       "   'precision': 0.7886178861788617,\n",
       "   'recall': 0.8818181818181818,\n",
       "   'support': 110},\n",
       "  '2': {'f1-score': 0.6161137440758294,\n",
       "   'precision': 0.5241935483870968,\n",
       "   'recall': 0.7471264367816092,\n",
       "   'support': 87},\n",
       "  '3': {'f1-score': 0.8509615384615385,\n",
       "   'precision': 0.944,\n",
       "   'recall': 0.774617067833698,\n",
       "   'support': 457},\n",
       "  '4': {'f1-score': 0.5833333333333334,\n",
       "   'precision': 0.4605263157894737,\n",
       "   'recall': 0.7954545454545454,\n",
       "   'support': 44},\n",
       "  'accuracy': 0.7893982808022922,\n",
       "  'classifier': 'Passive Agressive',\n",
       "  'macro avg': {'f1-score': 0.7207566604054436,\n",
       "   'precision': 0.6793344375888579,\n",
       "   'recall': 0.7997540579720086,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__C': 0.5,\n",
       "   'clf__fit_intercept': True,\n",
       "   'clf__loss': 'hinge',\n",
       "   'clf__max_iter': 15},\n",
       "  'vectorizer': 'Count',\n",
       "  'weighted avg': {'f1-score': 0.8019283213621846,\n",
       "   'precision': 0.8367105502637379,\n",
       "   'recall': 0.7893982808022922,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.8205128205128206,\n",
       "   'precision': 0.7804878048780488,\n",
       "   'recall': 0.8648648648648649,\n",
       "   'support': 111},\n",
       "  '2': {'f1-score': 0.5963636363636364,\n",
       "   'precision': 0.6612903225806451,\n",
       "   'recall': 0.543046357615894,\n",
       "   'support': 151},\n",
       "  '3': {'f1-score': 0.8514588859416445,\n",
       "   'precision': 0.856,\n",
       "   'recall': 0.8469656992084432,\n",
       "   'support': 379},\n",
       "  '4': {'f1-score': 0.556390977443609,\n",
       "   'precision': 0.4868421052631579,\n",
       "   'recall': 0.6491228070175439,\n",
       "   'support': 57},\n",
       "  'accuracy': 0.7679083094555874,\n",
       "  'classifier': 'SGDClassifier',\n",
       "  'macro avg': {'f1-score': 0.7061815800654276,\n",
       "   'precision': 0.696155058180463,\n",
       "   'recall': 0.7259999321766866,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__alpha': 1e-05,\n",
       "   'clf__max_iter': 20,\n",
       "   'clf__penalty': 'elasticnet'},\n",
       "  'vectorizer': 'Count',\n",
       "  'weighted avg': {'f1-score': 0.7672564980716349,\n",
       "   'precision': 0.7717234742853021,\n",
       "   'recall': 0.7679083094555874,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.8053097345132745,\n",
       "   'precision': 0.7398373983739838,\n",
       "   'recall': 0.883495145631068,\n",
       "   'support': 103},\n",
       "  '2': {'f1-score': 0.592274678111588,\n",
       "   'precision': 0.5564516129032258,\n",
       "   'recall': 0.6330275229357798,\n",
       "   'support': 109},\n",
       "  '3': {'f1-score': 0.8454882571075402,\n",
       "   'precision': 0.912,\n",
       "   'recall': 0.7880184331797235,\n",
       "   'support': 434},\n",
       "  '4': {'f1-score': 0.578125,\n",
       "   'precision': 0.4868421052631579,\n",
       "   'recall': 0.7115384615384616,\n",
       "   'support': 52},\n",
       "  'accuracy': 0.7722063037249284,\n",
       "  'classifier': 'SVM',\n",
       "  'macro avg': {'f1-score': 0.7052994174331007,\n",
       "   'precision': 0.6737827791350919,\n",
       "   'recall': 0.7540198908212583,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__C': 100, 'clf__gamma': 0.0001, 'clf__kernel': 'rbf'},\n",
       "  'vectorizer': 'Count',\n",
       "  'weighted avg': {'f1-score': 0.7800992065239296,\n",
       "   'precision': 0.7993986637717136,\n",
       "   'recall': 0.7722063037249284,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.7619047619047619,\n",
       "   'precision': 0.7154471544715447,\n",
       "   'recall': 0.8148148148148148,\n",
       "   'support': 108},\n",
       "  '2': {'f1-score': 0.4285714285714286,\n",
       "   'precision': 0.2903225806451613,\n",
       "   'recall': 0.8181818181818182,\n",
       "   'support': 44},\n",
       "  '3': {'f1-score': 0.8157303370786516,\n",
       "   'precision': 0.968,\n",
       "   'recall': 0.7048543689320388,\n",
       "   'support': 515},\n",
       "  '4': {'f1-score': 0.5046728971962617,\n",
       "   'precision': 0.35526315789473684,\n",
       "   'recall': 0.8709677419354839,\n",
       "   'support': 31},\n",
       "  'accuracy': 0.7363896848137536,\n",
       "  'classifier': 'ET',\n",
       "  'macro avg': {'f1-score': 0.6277198561877759,\n",
       "   'precision': 0.5822582232528607,\n",
       "   'recall': 0.802204685966039,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__max_features': 'sqrt'},\n",
       "  'vectorizer': 'Count',\n",
       "  'weighted avg': {'f1-score': 0.7691817199877462,\n",
       "   'precision': 0.8589908941633965,\n",
       "   'recall': 0.7363896848137536,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.7339449541284403,\n",
       "   'precision': 0.6504065040650406,\n",
       "   'recall': 0.8421052631578947,\n",
       "   'support': 95},\n",
       "  '2': {'f1-score': 0.5964912280701755,\n",
       "   'precision': 0.5483870967741935,\n",
       "   'recall': 0.6538461538461539,\n",
       "   'support': 104},\n",
       "  '3': {'f1-score': 0.847001223990208,\n",
       "   'precision': 0.9226666666666666,\n",
       "   'recall': 0.7828054298642534,\n",
       "   'support': 442},\n",
       "  '4': {'f1-score': 0.5714285714285715,\n",
       "   'precision': 0.5,\n",
       "   'recall': 0.6666666666666666,\n",
       "   'support': 57},\n",
       "  'accuracy': 0.7621776504297995,\n",
       "  'classifier': 'BernoulliNB',\n",
       "  'macro avg': {'f1-score': 0.687216494404349,\n",
       "   'precision': 0.6553650668764752,\n",
       "   'recall': 0.7363558783837421,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__alpha': 1e-05},\n",
       "  'vectorizer': 'Tfidf',\n",
       "  'weighted avg': {'f1-score': 0.7717848537773647,\n",
       "   'precision': 0.7953288576179968,\n",
       "   'recall': 0.7621776504297995,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.8429752066115702,\n",
       "   'precision': 0.8292682926829268,\n",
       "   'recall': 0.8571428571428571,\n",
       "   'support': 119},\n",
       "  '2': {'f1-score': 0.6754385964912281,\n",
       "   'precision': 0.6209677419354839,\n",
       "   'recall': 0.7403846153846154,\n",
       "   'support': 104},\n",
       "  '3': {'f1-score': 0.8709677419354838,\n",
       "   'precision': 0.936,\n",
       "   'recall': 0.814385150812065,\n",
       "   'support': 431},\n",
       "  '4': {'f1-score': 0.6166666666666667,\n",
       "   'precision': 0.4868421052631579,\n",
       "   'recall': 0.8409090909090909,\n",
       "   'support': 44},\n",
       "  'accuracy': 0.8123209169054442,\n",
       "  'classifier': 'Passive Agressive',\n",
       "  'macro avg': {'f1-score': 0.7515120529262372,\n",
       "   'precision': 0.7182695349703921,\n",
       "   'recall': 0.813205428562157,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__C': 1.0,\n",
       "   'clf__fit_intercept': False,\n",
       "   'clf__loss': 'hinge',\n",
       "   'clf__max_iter': 15},\n",
       "  'vectorizer': 'Tfidf',\n",
       "  'weighted avg': {'f1-score': 0.8210316529074375,\n",
       "   'precision': 0.8425510381405982,\n",
       "   'recall': 0.8123209169054442,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.84,\n",
       "   'precision': 0.8536585365853658,\n",
       "   'recall': 0.8267716535433071,\n",
       "   'support': 127},\n",
       "  '2': {'f1-score': 0.6536585365853659,\n",
       "   'precision': 0.5403225806451613,\n",
       "   'recall': 0.8271604938271605,\n",
       "   'support': 81},\n",
       "  '3': {'f1-score': 0.865525672371638,\n",
       "   'precision': 0.944,\n",
       "   'recall': 0.7990970654627539,\n",
       "   'support': 443},\n",
       "  '4': {'f1-score': 0.6341463414634146,\n",
       "   'precision': 0.5131578947368421,\n",
       "   'recall': 0.8297872340425532,\n",
       "   'support': 47},\n",
       "  'accuracy': 0.8094555873925502,\n",
       "  'classifier': 'SGDClassifier',\n",
       "  'macro avg': {'f1-score': 0.7483326376051046,\n",
       "   'precision': 0.7127847529918423,\n",
       "   'recall': 0.8207041117189438,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__alpha': 1e-05,\n",
       "   'clf__max_iter': 30,\n",
       "   'clf__penalty': 'l2'},\n",
       "  'vectorizer': 'Tfidf',\n",
       "  'weighted avg': {'f1-score': 0.820715032052766,\n",
       "   'precision': 0.8517065676665201,\n",
       "   'recall': 0.8094555873925502,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.8354430379746836,\n",
       "   'precision': 0.8048780487804879,\n",
       "   'recall': 0.868421052631579,\n",
       "   'support': 114},\n",
       "  '2': {'f1-score': 0.6637554585152838,\n",
       "   'precision': 0.6129032258064516,\n",
       "   'recall': 0.7238095238095238,\n",
       "   'support': 105},\n",
       "  '3': {'f1-score': 0.8681592039800995,\n",
       "   'precision': 0.9306666666666666,\n",
       "   'recall': 0.8135198135198135,\n",
       "   'support': 429},\n",
       "  '4': {'f1-score': 0.6349206349206348,\n",
       "   'precision': 0.5263157894736842,\n",
       "   'recall': 0.8,\n",
       "   'support': 50},\n",
       "  'accuracy': 0.8080229226361032,\n",
       "  'classifier': 'SVM',\n",
       "  'macro avg': {'f1-score': 0.7505695838476755,\n",
       "   'precision': 0.7186909326818226,\n",
       "   'recall': 0.801437597490229,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__C': 1000, 'clf__gamma': 0.001, 'clf__kernel': 'rbf'},\n",
       "  'vectorizer': 'Tfidf',\n",
       "  'weighted avg': {'f1-score': 0.8153598276887006,\n",
       "   'precision': 0.833356340607933,\n",
       "   'recall': 0.8080229226361032,\n",
       "   'support': 698}},\n",
       " {'1': {'f1-score': 0.7798165137614679,\n",
       "   'precision': 0.6910569105691057,\n",
       "   'recall': 0.8947368421052632,\n",
       "   'support': 95},\n",
       "  '2': {'f1-score': 0.4352941176470589,\n",
       "   'precision': 0.29838709677419356,\n",
       "   'recall': 0.8043478260869565,\n",
       "   'support': 46},\n",
       "  '3': {'f1-score': 0.8097345132743362,\n",
       "   'precision': 0.976,\n",
       "   'recall': 0.6918714555765595,\n",
       "   'support': 529},\n",
       "  '4': {'f1-score': 0.5,\n",
       "   'precision': 0.34210526315789475,\n",
       "   'recall': 0.9285714285714286,\n",
       "   'support': 28},\n",
       "  'accuracy': 0.7363896848137536,\n",
       "  'classifier': 'ET',\n",
       "  'macro avg': {'f1-score': 0.6312112861707158,\n",
       "   'precision': 0.5768873176252985,\n",
       "   'recall': 0.829881888085052,\n",
       "   'support': 698},\n",
       "  'model': 'baseline',\n",
       "  'parameters': {'clf__max_features': 'sqrt'},\n",
       "  'vectorizer': 'Tfidf',\n",
       "  'weighted avg': {'f1-score': 0.7685611113771176,\n",
       "   'precision': 0.8671334675130358,\n",
       "   'recall': 0.7363896848137536,\n",
       "   'support': 698}}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from src.analysis import classifier\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "from src.analysis.classifier import *\n",
    "\n",
    "def get_scores(args):\n",
    "    a = classifier.classifier_analyzer(path_to_data=args.data_path, path_to_embeddings=args.word_embedding_path, dataset=args.dataset)\n",
    "    class_report, results = a.gridsearch_with_classifiers()\n",
    "\n",
    "    fname_accuracy = '{}embeddings_classreport_{}_embed_size_{}.json'.format(args.outputpath, args.dataset.split('.')[0], args.word_embedding_sample_size)\n",
    "    fname_predictions = '{}embeddings_true_predicted_{}_embed_size_{}.json'.format(args.outputpath, args.dataset.split('.')[0], args.word_embedding_sample_size)\n",
    "\n",
    "    with open(fname_accuracy, mode = 'w') as fo:\n",
    "        json.dump(class_report, fo)\n",
    "        \n",
    "    df = clean_df_true_pred(results)\n",
    "    df.to_json(fname_predictions)\n",
    "\n",
    "\n",
    "def get_scores_baseline(args):\n",
    "    a = classifier.classifier_analyzer(path_to_data=args.data_path, path_to_embeddings=args.word_embedding_path, dataset=args.dataset)\n",
    "    class_report, results = a.gridsearch_with_classifiers_baseline()\n",
    "\n",
    "    fname_accuracy = '{}baseline_classreport_{}_embed_size_{}.json'.format(args.outputpath, args.dataset.split('.')[0], args.word_embedding_sample_size)\n",
    "    fname_true_predicted = '{}baseline_true_predicted_{}_embed_size_{}.json'.format(args.outputpath, args.dataset.split('.')[0], args.word_embedding_sample_size)\n",
    "\n",
    "    with open(fname_accuracy, mode = 'w') as fo:\n",
    "        json.dump(class_report, fo)\n",
    "\n",
    "    df = clean_df_true_pred(results)\n",
    "    df.to_json(fname_true_predicted)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Compute accuracy score of pretrained word embedding models')\n",
    "    parser.add_argument('--word_embedding_sample_size', type=str, required=False, default = 'large', help='Size of sample of pretrained word embedding (small or large)')\n",
    "    parser.add_argument('--word_embedding_path', type=str, required=True, help='Path of pretrained word embedding.')\n",
    "    parser.add_argument('--data_path', type=str, required=False, default='data/', help='Path of dataset with annotated data to be classified')\n",
    "    parser.add_argument('--dataset', type=str, required=False, default='dataset_vermeer.pkl', help='Path of dataset with annotated data to be classified')\n",
    "    parser.add_argument('--outputpath', type=str, required=False, default='output/output', help='Path of output file (CSV formatted classification scores)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('Arguments:')\n",
    "    print('word_embedding_sample_size:', args.word_embedding_sample_size)\n",
    "    print('word_embedding_path:', args.word_embedding_path)\n",
    "    print('data_path:', args.data_path)\n",
    "    print('dataset:', args.dataset)\n",
    "    print('outputpath:', args.outputpath)\n",
    "    print()\n",
    "    \n",
    "    get_scores(args)\n",
    "    get_scores_baseline(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
